{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datapipe\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapipe - functions for data back-end / manipulations\n",
    "\n",
    "* This is the module for creating the data pipeline.\n",
    "* It should also be used to perform the data pre-processing and caching.\n",
    "\n",
    "* **NOTE: This is currently broken because the iCloud Drive has not downloaded the files as yet.**\n",
    "\n",
    "## Pre-process (prep) the data - do this ONCE and only ONCE - they put in some re-useable form\n",
    "\n",
    "0. Think about capturing walk metadata / do on a per-overall-walk basis \n",
    "1. Need to \"extract\" the data from the .fit files\n",
    "2. Clean/fix the data (e.g. allow for breaks in walk, change in order, not turning off walk at end)\n",
    "3. Concatenate into a single data-structure per overall walk \n",
    "4. Store in \"database\" e.g. sqlite, postgres?, files, or is Quilt sufficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pandas as pd\n",
    "import activityio as aio\n",
    "from dateutil.parser import parse\n",
    "import datetime as dt\n",
    "import sqlite3 as sql\n",
    "from pathlib import Path\n",
    "import tomli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FIT_FILE_PATH = 'icloud/Data/HealthFit/FIT' \n",
    "WALK_DATABASE_NAME = 'emmaus_walking.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dir = Path.home()/RAW_FIT_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = Path(WALK_DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = []\n",
    "for path in sorted(fit_dir.iterdir()):\n",
    "        if path.is_dir():\n",
    "            walks.append([path.parts[-1], 'Name to be defined'])\n",
    "walks.append(['ALL', 'All Walks'])\n",
    "walks_df = pd.DataFrame(walks, columns=['walk_shortname', 'walk_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['B2M', 'Name to be defined'],\n",
       " ['B2W', 'Name to be defined'],\n",
       " ['D2C', 'Name to be defined'],\n",
       " ['GNW', 'Name to be defined'],\n",
       " ['GTL', 'Name to be defined'],\n",
       " ['GWW', 'Name to be defined'],\n",
       " ['OLD', 'Name to be defined'],\n",
       " ['SNM', 'Name to be defined'],\n",
       " ['STM', 'Name to be defined'],\n",
       " ['WNG', 'Name to be defined'],\n",
       " ['ALL', 'All Walks']]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks_df.to_json('walks_TBD.json', orient='table', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B2M: Name to be defined',\n",
       " 'B2W: Name to be defined',\n",
       " 'D2C: Name to be defined',\n",
       " 'GNW: Name to be defined',\n",
       " 'GTL: Name to be defined',\n",
       " 'GWW: Name to be defined',\n",
       " 'OLD: Name to be defined',\n",
       " 'SNM: Name to be defined',\n",
       " 'STM: Name to be defined',\n",
       " 'WNG: Name to be defined',\n",
       " 'ALL: All Walks']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "[shortname + ': ' + name for shortname, name in pd.read_json('walks_TBD.json', orient='table').values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_DETAILS_FILE = 'walk_details.toml'\n",
    "walk_details = Path('../' + WALK_DETAILS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Path('../walk_details.toml')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "walk_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(walk_details, encoding=\"utf-8\") as f:\n",
    "    walk_details_dict = tomli.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'walks': [{'short_name': 'B2M', 'name': 'Bondi to Manly'},\n",
       "  {'short_name': 'B2W', 'name': 'Bondi to Wollongong'},\n",
       "  {'short_name': 'D2C', 'name': 'Drummoyne to Cockatoo'},\n",
       "  {'short_name': 'GNW', 'name': 'Great North Walk'},\n",
       "  {'short_name': 'GTL', 'name': 'Gladesville Loop'},\n",
       "  {'short_name': 'GNW', 'name': 'Great North Walk'},\n",
       "  {'short_name': 'GWW', 'name': 'Great West Walk', 'status': 'incomplete'},\n",
       "  {'short_name': 'OLD', 'name': 'Old Bar'},\n",
       "  {'short_name': 'STM', 'name': \"St Michael's Golf Course\"},\n",
       "  {'short_name': 'SNM', 'name': 'Snowy Mountains (Thredo)'},\n",
       "  {'short_name': 'WNG',\n",
       "   'name': 'Newcastle to Sydney',\n",
       "   'status': 'incomplete'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "walk_details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                walks\n",
       "0     {'short_name': 'B2M', 'name': 'Bondi to Manly'}\n",
       "1   {'short_name': 'B2W', 'name': 'Bondi to Wollon...\n",
       "2   {'short_name': 'D2C', 'name': 'Drummoyne to Co...\n",
       "3   {'short_name': 'GNW', 'name': 'Great North Walk'}\n",
       "4   {'short_name': 'GTL', 'name': 'Gladesville Loop'}\n",
       "5   {'short_name': 'GNW', 'name': 'Great North Walk'}\n",
       "6   {'short_name': 'GWW', 'name': 'Great West Walk...\n",
       "7            {'short_name': 'OLD', 'name': 'Old Bar'}\n",
       "8   {'short_name': 'STM', 'name': 'St Michael's Go...\n",
       "9   {'short_name': 'SNM', 'name': 'Snowy Mountains...\n",
       "10  {'short_name': 'WNG', 'name': 'Newcastle to Sy..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>walks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{'short_name': 'B2M', 'name': 'Bondi to Manly'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{'short_name': 'B2W', 'name': 'Bondi to Wollon...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{'short_name': 'D2C', 'name': 'Drummoyne to Co...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{'short_name': 'GNW', 'name': 'Great North Walk'}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{'short_name': 'GTL', 'name': 'Gladesville Loop'}</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>{'short_name': 'GNW', 'name': 'Great North Walk'}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>{'short_name': 'GWW', 'name': 'Great West Walk...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>{'short_name': 'OLD', 'name': 'Old Bar'}</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>{'short_name': 'STM', 'name': 'St Michael's Go...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>{'short_name': 'SNM', 'name': 'Snowy Mountains...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>{'short_name': 'WNG', 'name': 'Newcastle to Sy...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "pd.DataFrame(walk_details_dict, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_from_walk_files():\n",
    "\n",
    "    # Get proper paths for files/db sorted & keep backup of previous .db\n",
    "\n",
    "    fit_dir = Path.home()/RAW_FIT_FILE_PATH\n",
    "    db_file = Path('../' + WALK_DATABASE_NAME)\n",
    "    \n",
    "    if db_file.is_file():\n",
    "        print('Deleting existing database')\n",
    "        db_file.unlink()\n",
    "\n",
    "    db_conn = sql.connect(db_file)\n",
    "    print('Created: ' + db_file.resolve().as_posix())\n",
    "    \n",
    "    for path in fit_dir.iterdir():\n",
    "        if path.is_dir():\n",
    "            walk_name = path.parts[-1]\n",
    "            print(walk_name)\n",
    "            walk_data, walk_date, walk_files, points, walk_stats = load_and_cache_raw_walk_data(walk_name, 1, db_conn)\n",
    "\n",
    "            # create table of walk meta-data\n",
    "\n",
    "            walk_meta = pd.DataFrame([walk_name, walk_date, walk_stats])\n",
    "            try:\n",
    "                walk_meta.to_sql('walk_meta', db_conn, if_exists='append', index=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    db_conn.close()\n",
    "    return db_file, walk_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_walk_stats(walk_data):\n",
    "    total_time = dt.timedelta(0)\n",
    "    total_distance = 0\n",
    "\n",
    "    for iHike, hike in enumerate(walk_data):\n",
    "        total_time += hike.index.max()\n",
    "        # print(iHike+1, walk_date[iHike], hike.index.max(), hike['dist'].max() / 1e3)\n",
    "        total_distance += hike['dist'].max()\n",
    "    total_distance /= 1e3\n",
    "\n",
    "    start_coord = walk_data[0][['lat', 'lon']].iloc[0].tolist()\n",
    "    end_coord = walk_data[-1][['lat', 'lon']].iloc[-1].tolist()\n",
    "    return total_time, total_distance, start_coord, end_coord\n",
    "\n",
    "\n",
    "# TODO: use st.cache() and also look to pre-load and cache/feather data (or similar) - NB: use of @st.cache() below didn't work\n",
    "def load_and_cache_raw_walk_data(walk_name, sample_freq, conn):\n",
    "    RAW_FIT_FILE_PATH = 'icloud/Data/HealthFit/FIT' \n",
    "    fit_dir = Path.home()/RAW_FIT_FILE_PATH\n",
    "    data_dir = fit_dir/walk_name[0:3]\n",
    "    print(data_dir.ls())\n",
    "    data_files = [file for file in os.listdir(data_dir) if file.endswith('.fit')]\n",
    "    walk_files = sorted(data_files)\n",
    "    print(walk_files)\n",
    "\n",
    "    walk_data = []\n",
    "    walk_date = []\n",
    "\n",
    "    for iFile, file in enumerate(walk_files):\n",
    "        print(file)\n",
    "        if Path(file).suffix == '.icloud':\n",
    "            print('Undownloaded files in iCloud Drive - STOP')\n",
    "            return False\n",
    "        walk_df = pd.DataFrame(aio.read(data_dir + file))\n",
    "        if len(walk_df) > 1:\n",
    "            walk_data.append(walk_df)\n",
    "            walk_date.append(parse(file[0:17]))\n",
    "            walk_df['WalkName'] = walk_name\n",
    "            walk_df['WalkNumber'] = iFile\n",
    "            walk_df[['alt', 'dist', 'lat', 'lon', 'speed', 'WalkName', 'WalkNumber']].to_sql('walks', conn, if_exists='append', index=False)\n",
    "               \n",
    "    total_time, total_distance, start_coord, end_coord = calc_walk_stats(walk_data)\n",
    "    walk_stats = [total_time, total_distance, start_coord, end_coord]\n",
    "    #print(start_coord)\n",
    "    walk_merged = pd.concat(walk_data)\n",
    "    points = walk_merged[['lat', 'lon']].values.tolist()\n",
    "    points = [tuple(point) for ipoint, point in enumerate(points) if ipoint % sample_freq == 0]\n",
    "    return walk_data, walk_date, walk_files, points, walk_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2939222685.py, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/_6/5btwyp417kb5tky1dhxm68000000gn/T/ipykernel_23339/2939222685.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    db_file, walk_meta = create_database_from_walk_files()\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_file, walk_meta = create_database_from_walk_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Path('emmaus_walking.db')"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "db_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_walk_datafile_for_app(db_file, n_rows_used=5):\n",
    "    # read in all of the walks data and sample at an appropriate frequency and cache for faster use in the app\n",
    "    db_conn = sql.connect(db_file)\n",
    "    walk_df = pd.read_sql_query('SELECT * FROM walks', db_conn)\n",
    "\n",
    "    UNUSED_COLUMNS = ['dist', 'speed']\n",
    "\n",
    "    walk_df.drop(UNUSED_COLUMNS, axis=1, inplace=True)\n",
    "    walk_df.dropna(inplace=True)      # TODO: Check why there are a few NaNs\n",
    "    walk_df = walk_df.iloc[::n_rows_used].reset_index()    # downsample\n",
    "\n",
    "    walk_df.to_feather(Path(db_file.as_posix().replace('.db', '.cache.feather')))\n",
    "    \n",
    "    return walk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1629182466.py, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/_6/5btwyp417kb5tky1dhxm68000000gn/T/ipykernel_23235/1629182466.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    walk_df = create_walk_datafile_for_app(db_file, 10)\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    walk_df = create_walk_datafile_for_app(db_file, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'walk_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_6/5btwyp417kb5tky1dhxm68000000gn/T/ipykernel_23235/2366156424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwalk_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwalk_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'walk_df' is not defined"
     ]
    }
   ],
   "source": [
    "walk_df[walk_df['lat'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Path('emmaus_walking.cache.feather')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "Path(db_file.as_posix().replace('.db', '.cache.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2088894046.py, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/_6/5btwyp417kb5tky1dhxm68000000gn/T/ipykernel_23331/2088894046.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    walk_df = pd.read_feather(Path(db_file.as_posix().replace('.db', '.cache.feather')))\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    walk_df = pd.read_feather(Path(db_file.as_posix().replace('.db', '.cache.feather')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'walk_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_6/5btwyp417kb5tky1dhxm68000000gn/T/ipykernel_22698/609809861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwalk_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'walk_df' is not defined"
     ]
    }
   ],
   "source": [
    "walk_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'walk_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_6/5btwyp417kb5tky1dhxm68000000gn/T/ipykernel_22698/362976362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwalk_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WalkName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'walk_df' is not defined"
     ]
    }
   ],
   "source": [
    "walk_df['WalkName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('emmaus_walking_py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "a22d2645d3afe98160956a5bd9591f63a934537cdda62e90bef32e6cf34fead0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}